{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dfbc76b",
   "metadata": {},
   "source": [
    "### ETL for pickup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623218ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "import logging\n",
    "\n",
    "postgresql_jdbc_jar = r\"C:/Program Files/PostgreSQL/17/postgresql-42.7.4.jar\"\n",
    "spark = SparkSession.builder.appName('ETL')\\\n",
    "                            .config(\"spark.jars\", postgresql_jdbc_jar) \\\n",
    "                            .config(\"spark.driver.extraClassPath\", postgresql_jdbc_jar) \\\n",
    "                            .config(\"spark.driver.memory\", \"8g\")\\\n",
    "                            .config(\"spark.executor.memory\", \"8g\")\\\n",
    "                            .config(\"spark.executor.cores\", \"4\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da48207",
   "metadata": {},
   "source": [
    "### Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8004c0c-83f7-461c-9e74-e30008176d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Pickup_ETL_Pipeline:Splitting data into train (80%) and test (20%)...\n",
      "INFO:Pickup_ETL_Pipeline:Train Data Count: 799139, Test Data Count: 1999\n",
      "INFO:Pickup_ETL_Pipeline:Preprocessing data: Handling missing values...\n",
      "INFO:Pickup_ETL_Pipeline:Missing values handled successfully.\n",
      "INFO:Pickup_ETL_Pipeline:Applying feature transformation...\n",
      "INFO:Pickup_ETL_Pipeline:Feature transformation completed successfully.\n",
      "INFO:Pickup_ETL_Pipeline:Loading regression model for ETA prediction...\n",
      "INFO:Pickup_ETL_Pipeline:Loading classification model...\n",
      "INFO:Pickup_ETL_Pipeline:Columns in Test Data Before Prediction: ['pickup_time', 'accept_time', 'is_delayed', 'speed_status', 'pca1', 'pca2', 'pca3', 'pca4', 'pca5', 'pca6', 'pca7', 'pca8', 'pca9', 'pickup_eta_minutes', 'is_delayed_indexed', 'speed_status_indexed', 'features', 'scaled_features', 'poly_features', 'predicted_eta']\n",
      "INFO:Pickup_ETL_Pipeline:Applying classification model for delay prediction...\n",
      "INFO:Pickup_ETL_Pipeline:Columns in Test Data After Prediction: ['pickup_time', 'accept_time', 'is_delayed', 'speed_status', 'pca1', 'pca2', 'pca3', 'pca4', 'pca5', 'pca6', 'pca7', 'pca8', 'pca9', 'pickup_eta_minutes', 'is_delayed_indexed', 'speed_status_indexed', 'features', 'scaled_features', 'poly_features', 'predicted_eta', 'rawPrediction', 'probability', 'prediction']\n",
      "INFO:Pickup_ETL_Pipeline:Delay prediction completed.\n",
      "INFO:Pickup_ETL_Pipeline:Saving predictions to CSV...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o5720.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 765.0 failed 1 times, most recent failure: Lost task 0.0 in stage 765.0 (TID 558) (DESKTOP-9JO5RF2 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/Dusty/Downloads/Internship/Last-Mile-Delivery-Delays-and-Route-Optimization/data/ETL_predictions_pickup.csv.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`ProbabilisticClassificationModel$$Lambda$4078/0x00000007c161d828`: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 17 more\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 11, y.size = 10\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:123)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.$anonfun$margin$1(LogisticRegression.scala:1151)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.$anonfun$margin$1$adapted(LogisticRegression.scala:1150)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1241)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1060)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:121)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/Dusty/Downloads/Internship/Last-Mile-Delivery-Delays-and-Route-Optimization/data/ETL_predictions_pickup.csv.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`ProbabilisticClassificationModel$$Lambda$4078/0x00000007c161d828`: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 17 more\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 11, y.size = 10\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:123)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.$anonfun$margin$1(LogisticRegression.scala:1151)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.$anonfun$margin$1$adapted(LogisticRegression.scala:1150)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1241)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1060)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:121)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 168\u001b[0m\n\u001b[0;32m    166\u001b[0m test_data \u001b[38;5;241m=\u001b[39m predict_eta(test_data, regression_model_path)  \u001b[38;5;66;03m# Predict ETA\u001b[39;00m\n\u001b[0;32m    167\u001b[0m test_data \u001b[38;5;241m=\u001b[39m predict_delay(test_data, classification_model_path)  \u001b[38;5;66;03m# Predict Delay\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[43msave_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_predictions_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Save Results\u001b[39;00m\n\u001b[0;32m    170\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPickup Pipeline Execution Completed Successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[46], line 145\u001b[0m, in \u001b[0;36msave_predictions\u001b[1;34m(df, output_path)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_predictions\u001b[39m(df, output_path):\n\u001b[0;32m    143\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving predictions to CSV...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 145\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicted_eta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_delayed_string\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_delayed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save as string\u001b[39;49;00m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelay_probability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madjusted_prediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep numerical version if needed\u001b[39;49;00m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions saved successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o5720.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 765.0 failed 1 times, most recent failure: Lost task 0.0 in stage 765.0 (TID 558) (DESKTOP-9JO5RF2 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/Dusty/Downloads/Internship/Last-Mile-Delivery-Delays-and-Route-Optimization/data/ETL_predictions_pickup.csv.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`ProbabilisticClassificationModel$$Lambda$4078/0x00000007c161d828`: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 17 more\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 11, y.size = 10\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:123)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.$anonfun$margin$1(LogisticRegression.scala:1151)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.$anonfun$margin$1$adapted(LogisticRegression.scala:1150)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1241)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1060)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:121)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/Dusty/Downloads/Internship/Last-Mile-Delivery-Delays-and-Route-Optimization/data/ETL_predictions_pickup.csv.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`ProbabilisticClassificationModel$$Lambda$4078/0x00000007c161d828`: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 17 more\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 11, y.size = 10\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:123)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.$anonfun$margin$1(LogisticRegression.scala:1151)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.$anonfun$margin$1$adapted(LogisticRegression.scala:1150)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1241)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1060)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:121)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA, StringIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel, LogisticRegressionModel\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.ml.tuning import TrainValidationSplitModel\n",
    "import logging\n",
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Pickup_ETL_Pipeline\").getOrCreate()\n",
    "\n",
    "# Setup Logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"Pickup_ETL_Pipeline\")\n",
    "\n",
    "# Task 1: Split df_model into Train (80%) and Test (20%)\n",
    "def split_data(df):\n",
    "    logger.info(\"Splitting data into train (80%) and test (20%)...\")\n",
    "    train_data, test_data = df.randomSplit([0.975, 0.0025], seed=42)\n",
    "    logger.info(f\"Train Data Count: {train_data.count()}, Test Data Count: {test_data.count()}\")\n",
    "    return test_data  # Using test data as new pickup data\n",
    "\n",
    "# Task 2: Preprocessing - Handle Missing Values\n",
    "def preprocess_data(df):\n",
    "    logger.info(\"Preprocessing data: Handling missing values...\")\n",
    "\n",
    "    # Identify Numeric Columns\n",
    "    numeric_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "\n",
    "    # Fill missing values only for numeric columns\n",
    "    for col_name in numeric_cols:\n",
    "        avg_value = df.selectExpr(f\"avg({col_name}) as avg\").collect()[0][\"avg\"]\n",
    "        if avg_value is not None:  # Ensure avg is not None\n",
    "            df = df.fillna({col_name: avg_value})\n",
    "\n",
    "    logger.info(\"Missing values handled successfully.\")\n",
    "    return df\n",
    "\n",
    "# Task 3: Feature Engineering - Select & Transform Features\n",
    "def feature_transformation(df):\n",
    "    logger.info(\"Applying feature transformation...\")\n",
    "\n",
    "    # Convert 'is_delayed' into numeric index\n",
    "    indexer_is_delayed = StringIndexer(inputCol=\"is_delayed\", outputCol=\"is_delayed_indexed\", handleInvalid=\"keep\")\n",
    "    df = indexer_is_delayed.fit(df).transform(df)\n",
    "\n",
    "    # Convert 'speed_status' into numeric index\n",
    "    indexer_speed_status = StringIndexer(inputCol=\"speed_status\", outputCol=\"speed_status_indexed\", handleInvalid=\"keep\")\n",
    "    df = indexer_speed_status.fit(df).transform(df)\n",
    "\n",
    "    # Feature Selection (Use indexed version of `is_delayed`)\n",
    "    feature_columns = [\n",
    "        \"pca1\", \"pca2\", \"pca3\", \"pca4\", \"pca5\", \"pca6\", \"pca7\", \"pca8\", \"pca9\", \n",
    "        \"speed_status_indexed\", \"is_delayed_indexed\"\n",
    "    ]\n",
    "\n",
    "    # Assemble Features\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    df = assembler.transform(df)\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "    scaler_model = scaler.fit(df)\n",
    "    df = scaler_model.transform(df)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(k=8, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "    pca_model = pca.fit(df)\n",
    "    df = pca_model.transform(df)\n",
    "\n",
    "    logger.info(\"Feature transformation completed successfully.\")\n",
    "    return df\n",
    "\n",
    "# Task 4: Load Regression Model & Predict ETA\n",
    "model_path_eta = r'C:\\Users\\Dusty\\Downloads\\Internship\\Last-Mile-Delivery-Delays-and-Route-Optimization\\notebooks\\regression_model_pickup'\n",
    "\n",
    "def predict_eta(df, model_path_eta):\n",
    "    logger.info(\"Loading regression model for ETA prediction...\")\n",
    "    \n",
    "    # Load trained Linear Regression model\n",
    "    regression_model = LinearRegressionModel.load(model_path_eta)\n",
    "\n",
    "    # Rename `pca_features` to `poly_features` (if applicable)\n",
    "    if \"pca_features\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"pca_features\", \"poly_features\")\n",
    "\n",
    "    # Make Predictions\n",
    "    df = regression_model.transform(df).withColumnRenamed(\"prediction\", \"predicted_eta\")\n",
    "    return df\n",
    "\n",
    "# Task 5: Load Classification Model & Predict Delay\n",
    "model_path_delay = r'C:\\Users\\Dusty\\Downloads\\Internship\\Last-Mile-Delivery-Delays-and-Route-Optimization\\notebooks\\classification_model_pickup'\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "def predict_delay(df, classification_model_path):\n",
    "    logger.info(\"Loading classification model...\")\n",
    "    classification_model = LogisticRegressionModel.load(classification_model_path)\n",
    "\n",
    "    logger.info(\"Columns in Test Data Before Prediction: \" + str(df.columns))\n",
    "\n",
    "    # Ensure 'features' column exists (rebuild it if necessary)\n",
    "    feature_cols = [\"speed_status_indexed\", \"pca1\", \"pca2\", \"pca3\", \"pca4\", \"pca5\", \"pca6\", \"pca7\", \"pca8\", \"pca9\"]\n",
    "    if \"features\" not in df.columns:\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        df = assembler.transform(df)\n",
    "\n",
    "    logger.info(\"Applying classification model for delay prediction...\")\n",
    "    df = classification_model.transform(df)\n",
    "\n",
    "    logger.info(\"Columns in Test Data After Prediction: \" + str(df.columns))\n",
    "\n",
    "    # Convert SparseVector probability column to Array\n",
    "    df = df.withColumn(\"probability_array\", vector_to_array(col(\"probability\")))\n",
    "\n",
    "    # Extract probability of delay\n",
    "    df = df.withColumn(\"delay_probability\", col(\"probability_array\")[1])\n",
    "\n",
    "    # Apply threshold for adjusted prediction\n",
    "    df = df.withColumn(\n",
    "        \"adjusted_prediction\",\n",
    "        when(col(\"delay_probability\") > 0.25, 1.0).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Convert numerical prediction to string labels\n",
    "    df = df.withColumn(\n",
    "        \"is_delayed_string\",\n",
    "        when(col(\"adjusted_prediction\") == 1.0, \"Delayed\").otherwise(\"On Time\")\n",
    "    )\n",
    "\n",
    "    logger.info(\"Delay prediction completed.\")\n",
    "    return df\n",
    "\n",
    "# Task 6: Save Predictions\n",
    "def save_predictions(df, output_path):\n",
    "    logger.info(\"Saving predictions to CSV...\")\n",
    "\n",
    "    df.select(\n",
    "        col(\"predicted_eta\"),\n",
    "        col(\"is_delayed_string\").alias(\"is_delayed\"),  # Save as string\n",
    "        col(\"delay_probability\"),\n",
    "        col(\"adjusted_prediction\")  # Keep numerical version if needed\n",
    "    ).write.csv(output_path, header=True)\n",
    "\n",
    "    logger.info(\"Predictions saved successfully.\")\n",
    "\n",
    "# Task 7: Execute Full Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Model Paths\n",
    "    regression_model_path = \"regression_model_pickup\"  # Replace with actual path\n",
    "    classification_model_path = \"classification_model_pickup\"  # Replace with actual path\n",
    "    output_predictions_path = r'C:\\Users\\Dusty\\Downloads\\Internship\\Last-Mile-Delivery-Delays-and-Route-Optimization\\data\\ETL_predictions_pickup.csv'\n",
    "\n",
    "\n",
    "    # Step-by-Step Execution\n",
    "    test_data = split_data(df_model)  # Split Data\n",
    "    test_data = preprocess_data(test_data)  # Preprocess Data\n",
    "    test_data = feature_transformation(test_data)  # Feature Engineering\n",
    "    test_data = predict_eta(test_data, regression_model_path)  # Predict ETA\n",
    "    test_data = predict_delay(test_data, classification_model_path)  # Predict Delay\n",
    "    save_predictions(test_data, output_predictions_path)  # Save Results\n",
    "\n",
    "    logger.info(\"Pickup Pipeline Execution Completed Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b99c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of training data:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No training summary available for this LogisticRegressionModel",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchema of training data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mclassification_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchema of test data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\ml\\classification.py:1581\u001b[0m, in \u001b[0;36mLogisticRegressionModel.summary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m LogisticRegressionTrainingSummary(\n\u001b[0;32m   1578\u001b[0m             \u001b[38;5;28msuper\u001b[39m(LogisticRegressionModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39msummary\n\u001b[0;32m   1579\u001b[0m         )\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo training summary available for this \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m   1583\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No training summary available for this LogisticRegressionModel"
     ]
    }
   ],
   "source": [
    "print(\"Schema of training data:\")\n",
    "classification_model.summary.predictions.printSchema()\n",
    "\n",
    "print(\"Schema of test data:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7625124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 14:27:19,327 - INFO - Extracting data from PostgreSQL...\n",
      "2025-03-16 14:27:19,933 - INFO - Data Extracted. Total Rows: 401124\n",
      "2025-03-16 14:27:19,933 - INFO - Applying Feature Transformation...\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "pca_1 does not exist. Available: order_id, pickup_time, accept_time, hour_of_day, day_of_week, month, pickup_eta_minutes, pickup_time_delay, pickup_distance_km, cluster, avg_pickup_time_minutes, pickup_order_count, city, city_order_count, is_delayed, speed_kmh, speed_status, speed_status_indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m feature_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_6\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_9\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed_status_indexed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     41\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39mfeature_columns, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m df_model \u001b[38;5;241m=\u001b[39m \u001b[43massembler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Standardization\u001b[39;00m\n\u001b[0;32m     45\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, withStd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, withMean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\ml\\base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\ml\\wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: pca_1 does not exist. Available: order_id, pickup_time, accept_time, hour_of_day, day_of_week, month, pickup_eta_minutes, pickup_time_delay, pickup_distance_km, cluster, avg_pickup_time_minutes, pickup_order_count, city, city_order_count, is_delayed, speed_kmh, speed_status, speed_status_indexed"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA, StringIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel, LogisticRegressionModel\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.ml.tuning import TrainValidationSplitModel\n",
    "import logging\n",
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import pickle\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ETA_ETL_Pipeline\").getOrCreate()\n",
    "\n",
    "# Task 1: Extract Data from PostgreSQL\n",
    "url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "logger.info(\"Extracting data from PostgreSQL...\")\n",
    "df_model = spark.read.jdbc(url=url, table=\"feature_engg_pickup_data\", properties=properties)\n",
    "df_model = df_model.sample(fraction=0.1, seed=42)\n",
    "logger.info(f\"Data Extracted. Total Rows: {df_model.count()}\")\n",
    "\n",
    "# Task 2: Feature Transformation (Encoding, Scaling, PCA)\n",
    "logger.info(\"Applying Feature Transformation...\")\n",
    "\n",
    "# Encode 'is_delayed' & 'speed_status'\n",
    "indexer_speed_status = StringIndexer(inputCol=\"speed_status\", outputCol=\"speed_status_indexed\", handleInvalid=\"keep\")\n",
    "df_model = indexer_speed_status.fit(df_model).transform(df_model)\n",
    "\n",
    "# Select Features\n",
    "feature_columns = [\"pca_1\", \"pca_2\", \"pca_3\", \"pca_4\", \"pca_5\", \"pca_6\", \"pca_7\", \"pca_8\", \"pca_9\", \"speed_status_indexed\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_model = assembler.transform(df_model)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(df_model)\n",
    "df_model = scaler_model.transform(df_model)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(k=9, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_model)\n",
    "df_model = pca_model.transform(df_model)\n",
    "\n",
    "logger.info(\"Feature Transformation Completed\")\n",
    "\n",
    "# Task 3: Load Pickle Model\n",
    "logger.info(\"Loading ETA Prediction Model...\")\n",
    "with open(\"eta_model_pickup.pkl\", \"rb\") as f:\n",
    "    eta_model = pickle.load(f)\n",
    "\n",
    "if eta_model:\n",
    "    logger.info(\"ETA Model Loaded Successfully\")\n",
    "else:\n",
    "    raise Exception(\"Failed to load ETA model\")\n",
    "\n",
    "# Task 4: Convert Spark DataFrame to NumPy Array for Prediction\n",
    "logger.info(\"Converting DataFrame to NumPy Array for Prediction...\")\n",
    "feature_data = np.array(df_model.select(\"pca_features\").rdd.map(lambda row: row[0]).collect())\n",
    "logger.info(f\"Feature Data Shape: {feature_data.shape}\")\n",
    "\n",
    "# Task 5: Make Predictions\n",
    "logger.info(\"Making Predictions on Data...\")\n",
    "predictions = eta_model.predict(feature_data)\n",
    "\n",
    "# Convert Predictions Back to Spark DataFrame\n",
    "predictions_df = spark.createDataFrame([(float(pred),) for pred in predictions], [\"predicted_eta\"])\n",
    "\n",
    "# Assign Unique IDs to Join Back with Original Data\n",
    "df_model = df_model.withColumn(\"id\", monotonically_increasing_id())\n",
    "predictions_df = predictions_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Join Predictions with Original Data\n",
    "df_final = df_model.join(predictions_df, on=\"id\").drop(\"id\")\n",
    "\n",
    "logger.info(\"ETA Predictions Completed\")\n",
    "\n",
    "# Task 6: Save Predictions\n",
    "output_path = \"ETA_predictions_pickup.csv\"\n",
    "df_final.select(col(\"predicted_eta\")).write.csv(output_path, header=True)\n",
    "logger.info(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "# Pipeline Completed\n",
    "logger.info(\"ETA Prediction Pipeline Completed Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb7410",
   "metadata": {},
   "source": [
    "#### Create Pickup Prediction API (FastAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize FastAPI App\n",
    "app = FastAPI()\n",
    "\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"FastAPI_Pickup\").getOrCreate()\n",
    "\n",
    "# Load Trained Pickup Models\n",
    "regression_model_pickup = LinearRegressionModel.load(\"regression_model_pickup\")\n",
    "classification_model_pickup = DecisionTreeClassificationModel.load(\"classification_model_pickup\")\n",
    "\n",
    "# Define Feature Columns for Pickup Model\n",
    "pickup_features = [\"speed_kmh\", \"pickup_distance_km\", \"pickup_eta_minutes\", \"avg_pickup_time_minutes\"]\n",
    "\n",
    "# Define Request Schema\n",
    "class PickupRequest(BaseModel):\n",
    "    speed_kmh: float\n",
    "    pickup_distance_km: float\n",
    "    pickup_eta_minutes: float\n",
    "    avg_pickup_time_minutes: float\n",
    "\n",
    "# Predict Pickup ETA & Delay\n",
    "@app.post(\"/predict/pickup\")\n",
    "def predict_pickup(data: PickupRequest):\n",
    "    input_data = [[data.speed_kmh, data.pickup_distance_km, data.pickup_eta_minutes, data.avg_pickup_time_minutes]]\n",
    "    df = spark.createDataFrame(input_data, pickup_features)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    assembler = VectorAssembler(inputCols=pickup_features, outputCol=\"features\")\n",
    "    df = assembler.transform(df)\n",
    "    \n",
    "    # Predict ETA\n",
    "    eta_prediction = regression_model_pickup.transform(df).select(\"prediction\").collect()[0][0]\n",
    "    \n",
    "    # Predict Delay (Binary Classification)\n",
    "    delay_prediction = classification_model_pickup.transform(df).select(\"prediction\").collect()[0][0]\n",
    "    \n",
    "    return {\n",
    "        \"predicted_pickup_eta\": round(eta_prediction, 2),\n",
    "        \"predicted_pickup_delay\": int(delay_prediction)  # 0 = No Delay, 1 = Delayed\n",
    "    }\n",
    "\n",
    "# Root Endpoint\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return {\"message\": \"Pickup Prediction API is running!\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f207355",
   "metadata": {},
   "source": [
    "### Latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf57251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extracting data from PostgreSQL...\n",
      "INFO:__main__:Data extraction completed.\n",
      "INFO:__main__:Performing feature engineering...\n",
      "INFO:__main__:Applying feature scaling...\n",
      "INFO:__main__:Feature scaling completed.\n",
      "INFO:__main__:Applying PCA with k=9...\n",
      "INFO:__main__:PCA transformation completed.\n",
      "INFO:__main__:Loading ETA prediction model...\n",
      "C:\\Users\\Dusty\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "INFO:__main__:ETA predictions completed.\n",
      "INFO:__main__:Loading Delay classification model...\n",
      "C:\\Users\\Dusty\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "INFO:__main__:Delay classification completed.\n",
      "INFO:__main__:Saving predictions to PostgreSQL...\n",
      "INFO:__main__:Predictions saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import logging\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# PostgreSQL Connection Details\n",
    "DB_URL = \"postgresql://postgres:root@localhost:5432/postgres\"\n",
    "TABLE_NAME = \"pickup_data\"  # Table with raw new data\n",
    "PREDICTIONS_TABLE = \"predictions_results\"  # Table to store predictions\n",
    "\n",
    "# Load Raw Data from PostgreSQL\n",
    "logger.info(\"Extracting data from PostgreSQL...\")\n",
    "engine = create_engine(DB_URL)\n",
    "df = pd.read_sql(f\"SELECT * FROM {TABLE_NAME}\", engine)\n",
    "df= df.sample(frac=0.01, random_state=42)\n",
    "# print(f\"Total number of rows: {len(df)}\")\n",
    "logger.info(\"Data extraction completed.\")\n",
    "\n",
    "# Feature Engineering\n",
    "logger.info(\"Performing feature engineering...\")\n",
    "\n",
    "# Convert datetime columns\n",
    "df[\"pickup_time\"] = pd.to_datetime(df[\"pickup_time\"])\n",
    "df[\"accept_time\"] = pd.to_datetime(df[\"accept_time\"])\n",
    "\n",
    "# Calculate new features\n",
    "df[\"pickup_eta_minutes\"] = (df[\"pickup_time\"] - df[\"accept_time\"]).dt.total_seconds() / 60\n",
    "# df[\"avg_pickup_time_minutes\"] = df[\"pickup_eta_minutes\"].mean()\n",
    "df[\"pickup_time_delay\"] = (df[\"pickup_time\"] - df[\"time_window_start\"]).dt.total_seconds() / 60\n",
    "# Define `is_delayed` based on a threshold (needed for delay prediction model)\n",
    "# time_threshold = 360  # Set the threshold in minutes\n",
    "df[\"is_delayed\"] = df[\"pickup_time_delay\"].apply(lambda x: \"Delayed\" if x > 0 else \"On Time\")\n",
    "\n",
    "# Drop unnecessary columns for model input\n",
    "df_model = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Apply Scaling\n",
    "logger.info(\"Applying feature scaling...\")\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_model)\n",
    "logger.info(\"Feature scaling completed.\")\n",
    "\n",
    "# Apply PCA with k=9\n",
    "logger.info(\"Applying PCA with k=9...\")\n",
    "pca = PCA(n_components=10)\n",
    "pca_features = pca.fit_transform(scaled_features)\n",
    "pca_columns = [f\"pca{i+1}\" for i in range(10)]\n",
    "df_pca = pd.DataFrame(pca_features, columns=pca_columns)\n",
    "logger.info(\"PCA transformation completed.\")\n",
    "\n",
    "# Load ETA Prediction Model\n",
    "logger.info(\"Loading ETA prediction model...\")\n",
    "with open(\"eta_model_pickup.pkl\", \"rb\") as f:\n",
    "    eta_model = pickle.load(f)\n",
    "\n",
    "# Predict ETA\n",
    "df_pca[\"predicted_eta\"] = eta_model.predict(df_pca)\n",
    "logger.info(\"ETA predictions completed.\")\n",
    "\n",
    "# Load Delay Classification Model\n",
    "logger.info(\"Loading Delay classification model...\")\n",
    "with open(\"delay_model_pickup.pkl\", \"rb\") as f:\n",
    "    delay_model = pickle.load(f)\n",
    "\n",
    "# Predict Delay\n",
    "df_pca[\"predicted_delay\"] = delay_model.predict(df_pca)\n",
    "logger.info(\"Delay classification completed.\")\n",
    "\n",
    "# Save Predictions to PostgreSQL\n",
    "logger.info(\"Saving predictions to PostgreSQL...\")\n",
    "df_pca[\"order_id\"] = df[\"order_id\"]  # Add back order_id for reference\n",
    "df_pca[[\"order_id\", \"predicted_eta\", \"predicted_delay\"]].to_sql(PREDICTIONS_TABLE, engine, if_exists=\"replace\", index=False)\n",
    "logger.info(\"Predictions saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c403f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555fa739",
   "metadata": {},
   "source": [
    "### Flask API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d80f66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.233:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug: * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dusty\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# Load Pretrained Models\n",
    "delay_model = \"eta_model_pickup.pkl\"\n",
    "eta_model = \"delay_model_pickup.pkl\"\n",
    "# Initialize Flask App\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Feature columns (adjust according to your pipeline)\n",
    "feature_cols = [\"speed_status_indexed\", \"pca1\", \"pca2\", \"pca3\", \"pca4\", \"pca5\", \"pca6\", \"pca7\", \"pca8\", \"pca9\"]\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get JSON input\n",
    "        input_data = request.get_json()\n",
    "\n",
    "        # Convert to Pandas DataFrame\n",
    "        df = pd.DataFrame([input_data])\n",
    "\n",
    "        # Convert to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "\n",
    "        # Assemble Features\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        spark_df = assembler.transform(spark_df)\n",
    "\n",
    "        # Make Predictions\n",
    "        delay_pred = delay_model.transform(spark_df).select(\"prediction\").collect()[0][0]\n",
    "        eta_pred = eta_model.transform(spark_df).select(\"prediction\").collect()[0][0]\n",
    "\n",
    "        # Return Predictions\n",
    "        return jsonify({\"delay_prediction\": int(delay_pred), \"eta_prediction\": round(eta_pred, 2)})\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de490d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delay Model Type: <class 'xgboost.sklearn.XGBRegressor'>\n",
      "ETA Model Type: <class 'sklearn.linear_model._logistic.LogisticRegression'>\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load and check models\n",
    "try:\n",
    "    delay_model = joblib.load(\"eta_model_pickup.pkl\")\n",
    "    eta_model = joblib.load(\"delay_model_pickup.pkl\")\n",
    "\n",
    "    print(f\"Delay Model Type: {type(delay_model)}\")\n",
    "    print(f\"ETA Model Type: {type(eta_model)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3eadb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
